# System Patterns ‚Äî APE 2026

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Å–æ–≥–ª–∞—à–µ–Ω–∏—è –∏ best practices –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞.

---

## Core Architectural Patterns

### 1. **Truth Boundary Pattern** (–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –¥–ª—è APE)

**–ü—Ä–æ–±–ª–µ–º–∞:** LLM –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É—é—Ç —á–∏—Å–ª–∞
**–†–µ—à–µ–Ω–∏–µ:** –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Layer  ‚îÇ ‚îÄ‚îÄ‚ñ∂  ‚îÇ  Code Layer  ‚îÇ ‚îÄ‚îÄ‚ñ∂  ‚îÇ  VEE Layer  ‚îÇ
‚îÇ             ‚îÇ      ‚îÇ              ‚îÇ      ‚îÇ             ‚îÇ
‚îÇ Generates:  ‚îÇ      ‚îÇ Generates:   ‚îÇ      ‚îÇ Executes:   ‚îÇ
‚îÇ - Plan      ‚îÇ      ‚îÇ - SQL        ‚îÇ      ‚îÇ - Query     ‚îÇ
‚îÇ - Code      ‚îÇ      ‚îÇ - Python     ‚îÇ      ‚îÇ - Script    ‚îÇ
‚îÇ - Explanation‚îÇ      ‚îÇ - API calls  ‚îÇ      ‚îÇ - Returns   ‚îÇ
‚îÇ             ‚îÇ      ‚îÇ              ‚îÇ      ‚îÇ  VerifiedFact‚îÇ
‚îÇ ‚ùå NO numbers‚îÇ      ‚îÇ ‚ùå NO numbers‚îÇ      ‚îÇ ‚úÖ Numbers  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Enforcement:**
- Truth Boundary Gate –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∂–¥–æ–µ —á–∏—Å–ª–æ –≤ —Ç–µ–∫—Å—Ç–µ
- –ï—Å–ª–∏ —á–∏—Å–ª–æ –Ω–µ –∏–º–µ–µ—Ç `FactRef` ‚Üí BLOCK
- –ï—Å–ª–∏ `FactRef` –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ `EvidenceBundle` ‚Üí BLOCK

**Code Example:**
```python
# ‚ùå BAD (LLM generating numbers directly)
def bad_volatility_report(symbol: str) -> str:
    return f"The volatility of {symbol} is 15.3%"  # Hallucination risk!

# ‚úÖ GOOD (Code-as-Truth)
def good_volatility_report(symbol: str) -> Report:
    # 1. Code generates fact
    fact = execute_in_vee(f"""
        import yfinance as yf
        data = yf.download('{symbol}', period='30d')
        volatility = data['Close'].pct_change().std()
        return {{'value': volatility, 'unit': 'ratio'}}
    """)

    # 2. LLM references fact, doesn't generate number
    return f"The volatility of {symbol} is {{{{FactRef({fact.id})|{fact.value:.1%}}}}}"
```

---

### 2. **Fail-Closed Pattern**

**–ü—Ä–∏–Ω—Ü–∏–ø:** –ü—Ä–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ ‚Üí UNCERTAIN, –Ω–µ –≤—ã–¥—É–º—ã–≤–∞—Ç—å

```python
def fail_closed_example(query: str, confidence: float, evidence_coverage: float) -> Response:
    if confidence < 0.70:
        return Response(
            verdict="UNCERTAIN",
            reason="Confidence below threshold (70%)",
            action="HITL_ESCALATE"
        )

    if evidence_coverage < 0.90:
        return Response(
            verdict="UNCERTAIN",
            reason="Evidence coverage insufficient (<90%)",
            action="REQUEST_MORE_DATA"
        )

    # Only return confident answer if all checks pass
    return Response(verdict="CONFIDENT", data=final_report)
```

**Triggers:**
- Confidence < 70%
- Evidence Coverage < 90%
- Sign flip –≤ sensitivity analysis
- Doubter verdict = NEED_HUMAN
- Temporal violation detected
- Truth Gate BLOCK

---

### 3. **Verifiable Execution Environment (VEE) Pattern**

**Isolation Layers:**
```yaml
Network:
  allowed_hosts:
    - api.stlouisfed.org  # FRED
    - query.yahooapis.com # Yahoo Finance
  blocked: everything else

Filesystem:
  read: /workspace (sandbox only)
  write: /workspace/output (sandbox only)
  blocked: /host/*

Resources:
  memory: 2GB hard limit
  timeout: 60 sec per execution
  cpu: 2 cores
```

**Security Checks (Pre-Execution):**
```python
def validate_code_safety(code: str) -> ValidationResult:
    forbidden_patterns = [
        r"import os",           # OS interaction
        r"import subprocess",   # Shell access
        r"open\(['\"]\/",       # Absolute path access
        r"__import__",          # Dynamic imports
        r"eval\(",              # Code injection
        r"exec\(",              # Code injection
    ]

    for pattern in forbidden_patterns:
        if re.search(pattern, code):
            return ValidationResult(safe=False, reason=f"Forbidden pattern: {pattern}")

    return ValidationResult(safe=True)
```

---

### 4. **Temporal Integrity Pattern** (–§–∏–Ω–∞–Ω—Å–æ–≤–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞)

**–ü—Ä–æ–±–ª–µ–º–∞:** Look-ahead bias ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–æ—à–ª–æ–º

```python
@dataclass
class VerifiedFact:
    asof_timestamp: datetime      # –ö–æ–≥–¥–∞ —Ñ–∞–∫—Ç –∏—Å—Ç–∏–Ω–µ–Ω
    publication_lag: timedelta    # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏

    @property
    def available_at(self) -> datetime:
        """–ö–æ–≥–¥–∞ —Ñ–∞–∫—Ç —Å—Ç–∞–ª –∏–∑–≤–µ—Å—Ç–µ–Ω —Ä—ã–Ω–∫—É"""
        return self.asof_timestamp + self.publication_lag

def check_temporal_integrity(fact: VerifiedFact, episode: EpisodeState) -> bool:
    if fact.available_at > episode.asof_timestamp:
        raise TemporalViolation(
            f"Fact available at {fact.available_at}, but query asof {episode.asof_timestamp}"
        )
    return True
```

**Publication Lags (Defaults):**
```python
PUBLICATION_LAGS = {
    "yfinance": timedelta(seconds=0),     # Real-time
    "fred_gdp": timedelta(days=90),       # Quarterly + revision
    "fred_cpi": timedelta(days=14),       # Monthly + 2 weeks
    "sec_10k": timedelta(days=45),        # 10-K filing deadline
    "earnings": timedelta(days=1),        # Next-day after call
}
```

---

### 5. **Multi-Agent Debate Pattern**

**Roles (–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ):**
```python
DEBATE_AGENTS = {
    "Bull": {
        "bias": "optimistic",
        "focus": "opportunities, positive trends",
        "model": "deepseek-v3"
    },
    "Bear": {
        "bias": "pessimistic",
        "focus": "risks, threats, data quality issues",
        "model": "deepseek-v3"
    },
    "Quant": {
        "bias": "neutral",
        "focus": "statistical significance, assumptions",
        "model": "deepseek-v3"
    }
}
```

**Consensus Calculation:**
```python
def calculate_consensus(reports: List[PanelReport]) -> ConsensusMetrics:
    confidences = [r.confidence for r in reports]

    # Vote entropy (0 = perfect agreement, 1 = max disagreement)
    vote_entropy = -sum(p * log(p) for p in confidences if p > 0) / log(len(confidences))

    # Pairwise contradiction rate
    contradictions = 0
    for i, r1 in enumerate(reports):
        for r2 in reports[i+1:]:
            if contradicts(r1.conclusion, r2.conclusion):
                contradictions += 1
    contradiction_rate = contradictions / (len(reports) * (len(reports) - 1) / 2)

    return ConsensusMetrics(
        vote_entropy=vote_entropy,
        contradiction_rate=contradiction_rate,
        confidence_penalty=vote_entropy * 0.5 + contradiction_rate * 0.5
    )
```

---

### 6. **Sensitivity Harness Pattern**

**–ü—Ä–æ–±–ª–µ–º–∞:** –í—ã–≤–æ–¥—ã —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º (–æ–∫–Ω–æ, –º–µ—Ç–æ–¥, outliers)

```python
def run_sensitivity_analysis(
    base_step: PlanStep,
    base_fact: VerifiedFact,
    parameters_to_vary: Dict[str, List[Any]]
) -> SensitivityReport:
    """
    Example:
    parameters_to_vary = {
        "window": ["20d", "30d", "60d", "90d"],
        "method": ["std", "ewm"],
    }
    """
    base_value = base_fact.value
    variations = []

    for param_name, param_values in parameters_to_vary.items():
        for param_value in param_values:
            variant_step = base_step.copy()
            variant_step.parameters[param_name] = param_value

            variant_fact = execute_step(variant_step)
            delta = (variant_fact.value - base_value) / base_value

            variations.append({
                "param": param_name,
                "value": param_value,
                "result": variant_fact.value,
                "delta_pct": delta
            })

    # Check for sign flips (–∫—Ä–∏—Ç–∏—á–Ω–æ!)
    sign_flips = any(
        (base_value > 0 and v["result"] < 0) or
        (base_value < 0 and v["result"] > 0)
        for v in variations
    )

    if sign_flips:
        # –†–µ–∑–∫–æ —Å–Ω–∏–∂–∞–µ–º confidence
        return SensitivityReport(
            stability="UNSTABLE",
            sign_flip=True,
            confidence_penalty=0.7,  # 70% penalty
            action="BLOCK_OR_WARN"
        )

    # Sensitivity score (—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å)
    deltas = [v["delta_pct"] for v in variations]
    sensitivity_score = 1.0 / (1.0 + np.std(deltas))

    return SensitivityReport(
        stability="STABLE" if sensitivity_score > 0.8 else "MODERATE",
        sign_flip=False,
        sensitivity_score=sensitivity_score,
        confidence_penalty=max(0, 1.0 - sensitivity_score)
    )
```

---

### 7. **Evidence Provenance Pattern**

**–ö–∞–∂–¥—ã–π —Ñ–∞–∫—Ç —Ö—Ä–∞–Ω–∏—Ç –ø–æ–ª–Ω—É—é –ø—Ä–æ–≤–µ–Ω–∞–Ω—Å:**
```python
@dataclass
class VerifiedFact:
    # Value
    value: Any
    unit: str  # "USD", "percent", "ratio", "count"

    # Provenance (–æ—Ç–∫—É–¥–∞ –ø—Ä–∏—à–ª–æ)
    source: {
        "name": str,          # "yfinance", "FRED", "clickhouse"
        "version": str,       # API/library version
        "query_hash": str,    # SHA256(query)
        "output_hash": str,   # SHA256(result)
    }
    method: str  # "sql_query", "python_script", "api_call"

    # Temporal
    asof_timestamp: datetime
    publication_lag: timedelta
    window_definition: Optional[str]  # "rolling_30d", "monthly", "ytd"

    # Audit
    created_at: datetime
    execution_log_ref: UUID  # –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–ª–Ω—ã–π –ª–æ–≥
```

**Lineage Graph (Neo4j):**
```cypher
// Episode –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ñ–∞–∫—Ç–æ–≤
(episode:Episode)-[:USED_FACT]->(fact:VerifiedFact)

// Derived —Ñ–∞–∫—Ç—ã –∑–∞–≤–∏—Å—è—Ç –æ—Ç verified —Ñ–∞–∫—Ç–æ–≤
(derived:DerivedFact)-[:DERIVED_FROM]->(fact:VerifiedFact)

// –§–∞–∫—Ç—ã —Å–æ–∑–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ execution logs
(fact:VerifiedFact)-[:CREATED_BY]->(log:ExecutionLog)

// Query –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏: "–æ—Ç–∫—É–¥–∞ –≤–∑—è–ª–æ—Å—å —ç—Ç–æ —á–∏—Å–ª–æ?"
MATCH path = (e:Episode {id: $episode_id})-[:USED_FACT*]->(f:VerifiedFact)
RETURN path
```

---

## Code Style Conventions

### Python Style
```python
# Pydantic models –¥–ª—è –≤—Å–µ—Ö schemas
from pydantic import BaseModel, Field

class VerifiedFact(BaseModel):
    id: UUID = Field(default_factory=uuid4)
    value: float
    unit: str

    class Config:
        frozen = True  # Immutable

# Type hints –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã
def execute_step(step: PlanStep, state: EpisodeState) -> VerifiedFact:
    ...

# Docstrings Google style
def validate_truth_boundary(text: str, facts: List[VerifiedFact]) -> ValidationResult:
    """Validate that all numbers in text are backed by VerifiedFacts.

    Args:
        text: LLM-generated report text
        facts: Available verified facts

    Returns:
        ValidationResult with verdict (PASS/BLOCK) and issues

    Raises:
        ValueError: If text is empty
    """
```

### Testing Conventions
```python
# Test naming: test_<component>_<scenario>_<expected>
def test_truth_gate_blocks_unverified_numbers():
    ...

def test_temporal_integrity_allows_fact_with_correct_lag():
    ...

# Fixtures –≤ conftest.py
@pytest.fixture
def sample_episode():
    return EpisodeState(
        episode_id=uuid4(),
        asof_timestamp=datetime(2024, 1, 15, tzinfo=timezone.utc),
        user_id="test_user"
    )

# –ú–æ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö API –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
@patch('yfinance.download')
def test_yfinance_adapter_handles_missing_data(mock_download):
    mock_download.return_value = None
    result = YFinanceAdapter().fetch_ohlcv("INVALID")
    assert result.status == "error"
```

---

## Error Handling Patterns

### Graceful Degradation
```python
def fetch_data_with_fallback(symbol: str) -> Optional[pd.DataFrame]:
    """Try primary source, fallback to secondary, fail gracefully."""
    try:
        return yfinance.download(symbol, period="30d")
    except Exception as e:
        logger.warning(f"YFinance failed for {symbol}: {e}, trying cache")
        try:
            return redis_cache.get(f"ohlcv:{symbol}")
        except Exception as e2:
            logger.error(f"Cache also failed: {e2}")
            return None  # Fail gracefully, don't crash
```

### Fail-Fast –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
```python
def validate_sandbox_isolation():
    """Pre-flight check: sandbox must be isolated or crash."""
    try:
        # Attempt to access host filesystem from sandbox
        result = docker_exec(container, "ls /host")
        if result.returncode == 0:
            raise SecurityViolation("Sandbox can access host filesystem!")
    except docker.errors.APIError:
        pass  # Expected ‚Äî sandbox is isolated, good
```

---

## Performance Patterns

### Caching Strategy
```python
# Level 1: In-memory (per-request)
@lru_cache(maxsize=128)
def parse_investigation_plan(plan_json: str) -> InvestigationPlan:
    return InvestigationPlan.parse_raw(plan_json)

# Level 2: Redis (cross-request, short TTL)
def get_market_data(symbol: str, date: str) -> pd.DataFrame:
    key = f"market:{symbol}:{date}"
    cached = redis.get(key)
    if cached:
        return pickle.loads(cached)

    data = yfinance.download(symbol, start=date, end=date)
    redis.setex(key, timedelta(hours=1), pickle.dumps(data))
    return data

# Level 3: Neo4j (persistent, invalidate on new episode)
def get_verified_fact(fact_id: UUID) -> VerifiedFact:
    # Facts are immutable, cache forever
    return neo4j.query("MATCH (f:VerifiedFact {id: $id}) RETURN f", id=fact_id)
```

### Parallel Execution
```python
# Multi-agent debate –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
with ThreadPoolExecutor(max_workers=3) as executor:
    futures = {
        executor.submit(run_agent, "Bull", evidence): "Bull",
        executor.submit(run_agent, "Bear", evidence): "Bear",
        executor.submit(run_agent, "Quant", evidence): "Quant",
    }
    reports = [future.result() for future in as_completed(futures)]
```

---

## Security Patterns

### Secrets Management
```python
# ‚ùå BAD
API_KEY = "sk-1234567890abcdef"

# ‚úÖ GOOD
from os import environ
API_KEY = environ.get("FRED_API_KEY")
if not API_KEY:
    raise ValueError("FRED_API_KEY environment variable not set")

# Docker Compose
services:
  app:
    environment:
      - FRED_API_KEY=${FRED_API_KEY}  # Injected from .env file
```

### Input Validation
```python
from pydantic import BaseModel, validator

class QueryRequest(BaseModel):
    query: str
    asof_timestamp: datetime

    @validator('query')
    def query_must_not_be_empty(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('Query cannot be empty')
        if len(v) > 1000:
            raise ValueError('Query too long (max 1000 chars)')
        return v

    @validator('asof_timestamp')
    def asof_must_be_past(cls, v):
        if v > datetime.now(timezone.utc):
            raise ValueError('asof_timestamp cannot be in the future')
        return v
```

---

## Logging \& Observability

### Structured Logging
```python
import structlog

logger = structlog.get_logger()

# ‚ùå BAD
print(f"Processing query: {query}")

# ‚úÖ GOOD
logger.info(
    "query_processing_started",
    episode_id=episode_id,
    query_length=len(query),
    asof_timestamp=asof_timestamp.isoformat()
)
```

### Metrics (Prometheus)
```python
from prometheus_client import Counter, Histogram

# Counters
queries_total = Counter('ape_queries_total', 'Total queries processed')
hallucinations_total = Counter('ape_hallucinations_total', 'Hallucinations detected')

# Histograms
query_duration = Histogram('ape_query_duration_seconds', 'Query processing time')

# Usage
@query_duration.time()
def process_query(query: str) -> Report:
    queries_total.inc()
    ...
    if hallucination_detected:
        hallucinations_total.inc()
```

---

## 8. **Prompt Methodology Pattern** (NEW - Week 4)

**–§–∏–ª–æ—Å–æ—Ñ–∏—è**: Prompt Compiler, –∞ –Ω–µ Prompt Library

**Reference**: See `promptMethodology.md` for full details

### Meta-Prompt Engine
```python
# –ï–¥–∏–Ω—ã–π META_PROMPT –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –ª—é–±—ã—Ö –∑–∞–¥–∞—á
class APEPromptCompiler:
    def compile(self, task_description: str, context: dict) -> str:
        # 1. CLASSIFY: code_generation | validation | analysis | debate
        # 2. SELECT techniques: Chain-of-Thought, Structured Output, etc.
        # 3. COMPOSE: 6 –±–ª–æ–∫–æ–≤ (ROLE, TASK, CONSTRAINTS, INPUT, OUTPUT, EDGE CASES)
        # 4. VALIDATE: testable, minimal, unambiguous
        return compiled_prompt
```

### Task Taxonomy (6 Categories)

| Category | APE Components | Status | Technique |
|----------|----------------|--------|-----------|
| A: Code Generation | PLAN Node | ‚úÖ v1 (need v2+ DSPy) | Structured Output + Constraints |
| B: Adversarial Validation | Doubter, TIM | ‚úÖ Implemented | Adversarial Framing + Checklist |
| C: Multi-Perspective | Debate (planned) | ‚è∏Ô∏è Week 5-6 | Role Assignment + Synthesis |
| D: Evaluation | Ground Truth | ‚úÖ Implemented | Rubric-based + CoT |
| E: Data Extraction | Truth Gate | ‚úÖ (deterministic) | Structured Output |
| F: Temporal Validation | TIM | ‚úÖ (rule-based) | Rule Injection + Checklist |

### Prompt Composition (6 Blocks)

Every APE prompt built from:
1. **ROLE** (1 sentence) ‚Äî only when domain expertise matters
2. **TASK** (imperative) ‚Äî ALWAYS
3. **CONSTRAINTS** (explicit prohibitions) ‚Äî high-risk tasks only
4. **INPUT FORMAT** ‚Äî when input != plain text
5. **OUTPUT FORMAT** (Pydantic schema) ‚Äî ALWAYS if code parses output
6. **EDGE CASES** ‚Äî real cases from production, not invented

### Hardcoded vs Compiled Strategy

| Component | Strategy | Rationale |
|-----------|----------|-----------|
| META_PROMPT | Hardcoded | Core compiler, changes rarely |
| PLAN Node | Hardcoded v1 ‚Üí DSPy v2+ | Critical path, will optimize |
| Doubter | Compiled (Week 4 Day 4) | Varies by fact type |
| Truth Gate | N/A (code) | Deterministic, no LLM |
| TIM | Hardcoded rules | Physical constants (T+90 etc.) |
| VEE | N/A (code) | Execution only |

### Anti-Patterns

‚ùå 1000 prompts library ‚Üí –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å
‚úÖ Meta-prompt + 6 categories ‚Üí –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å—ë

‚ùå Few-shot everywhere ‚Üí —Ä–∞–∑–¥—É–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
‚úÖ Few-shot —Ç–æ–ª—å–∫–æ –¥–ª—è non-obvious format

‚ùå "Be helpful and accurate" ‚Üí –ø—É—Å—Ç—ã–µ —Å–ª–æ–≤–∞
‚úÖ "NEVER output raw numbers" ‚Üí –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π constraint

‚ùå Hardcoded strings –≤ –∫–æ–¥–µ ‚Üí –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ A/B test
‚úÖ Configurable prompts ‚Üí —ç–≤–æ–ª—é—Ü–∏—è —Å –ø—Ä–æ–¥—É–∫—Ç–æ–º

### Prompt Lifecycle (TDD for Prompts)

```
v0: Meta-Prompt generates draft (2 min)
  ‚Üì
v1: TDD with 5 test cases (30 min)
  ‚Üì
v2: DSPy optimization [optional] (2-4 hours)
  ‚Üì
v3+: Production feedback loop (ongoing)
```

### Immediate Action Items (Week 4-5)

1. ‚úÖ Integrate methodology into memory bank (Week 4 Day 3)
2. üî¥ Create `APEPromptCompiler` (Week 4 Day 4-5)
3. üî¥ Refactor Doubter to use compiled prompts
4. üü° DSPy optimization for PLAN Node (Week 5)
5. üü° Add prompt versioning system

---

*–≠—Ç–æ—Ç —Ñ–∞–π–ª –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤*
*Last Updated: 2026-02-08 (Added Prompt Methodology Pattern)*
