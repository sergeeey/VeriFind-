# APE Prompt Methodology ‚Äî Memory Bank

**Version**: 1.0
**Last Updated**: 2026-02-08
**Source**: ape_prompt_methodology.md

---

## –§–∏–ª–æ—Å–æ—Ñ–∏—è: Prompt Compiler, –∞ –Ω–µ Prompt Library

**–ö–ª—é—á–µ–≤–æ–π –ø—Ä–∏–Ω—Ü–∏–ø**: –ú—ã –ù–ï –ø–∏—à–µ–º 1000 –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–¥ 1000 –∑–∞–¥–∞—á.
–ú—ã —Å—Ç—Ä–æ–∏–º –ö–û–ú–ü–ò–õ–Ø–¢–û–†, –∫–æ—Ç–æ—Ä—ã–π –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç.

---

## –£–†–û–í–ï–ù–¨ 1: META-PROMPT ENGINE

### Meta-Prompt (—è–¥—Ä–æ —Å–∏—Å—Ç–µ–º—ã)

```python
META_PROMPT = '''
You are APE Prompt Compiler. Given a TASK DESCRIPTION, you generate
the optimal system prompt for an LLM that will execute this task.

## YOUR METHODOLOGY (apply in order):

### Step 1: CLASSIFY the task
- What TYPE of task? (code_generation | analysis | validation | synthesis | debate)
- What DOMAIN? (financial | statistical | temporal | general)
- What OUTPUT? (structured_json | narrative | boolean | numerical)
- What RISK? (high = money decisions | medium = analysis | low = formatting)

### Step 2: SELECT techniques
Based on classification, choose from:
- Chain-of-Thought ‚Üí for multi-step reasoning
- Structured Output ‚Üí for parseable results (always if downstream code consumes it)
- Few-Shot ‚Üí ONLY if task has non-obvious format requirements
- Role Assignment ‚Üí ONLY if domain expertise matters
- Adversarial Framing ‚Üí ONLY if task is validation/audit
- Constraint Injection ‚Üí ALWAYS for high-risk tasks

### Step 3: COMPOSE the prompt
Structure:
1. ROLE (1 sentence ‚Äî who is the LLM in this context)
2. TASK (what exactly to do ‚Äî imperative, specific)
3. CONSTRAINTS (what NEVER to do ‚Äî explicit prohibitions)
4. INPUT FORMAT (what data the LLM receives)
5. OUTPUT FORMAT (exact schema or structure expected)
6. EDGE CASES (what to do when input is ambiguous/broken)

### Step 4: VALIDATE
- Is the prompt TESTABLE? (can we write an assertion against output?)
- Is it MINIMAL? (remove any sentence that doesn't change output)
- Is it UNAMBIGUOUS? (would 2 different LLMs interpret it the same way?)

## OUTPUT
Return the generated system prompt + reasoning for each choice made.
'''
```

---

## –£–†–û–í–ï–ù–¨ 2: TASK TAXONOMY (APE Components)

### –ö–∞—Ç–µ–≥–æ—Ä–∏—è A: Code Generation (PLAN Node)
```
–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- Output = executable code
- Risk = HIGH (–∫–æ–¥ –∏—Å–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ sandbox)
- Technique = Structured Output + Constraints + Few-Shot
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: –∑–∞–ø—Ä–µ—Ç raw numbers, only print key:value
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: Pydantic schema –¥–ª—è output

APE Components: PlanNode
Status: ‚úÖ IMPLEMENTED (hardcoded v1, need v2+ DSPy optimization)
```

### –ö–∞—Ç–µ–≥–æ—Ä–∏—è B: Adversarial Validation (Doubter, TIM)
```
–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- Output = verdict (pass/fail/conditional)
- Risk = HIGH (gate decision)
- Technique = Adversarial Framing + Chain-of-Thought + Checklist
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: DEFAULT = reject, –¥–æ–∫–∞–∑—ã–≤–∞–π —á—Ç–æ ok
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: explicit checklist –≤ –ø—Ä–æ–º–ø—Ç–µ

APE Components: DoubterAgent, TemporalIntegrityChecker
Status:
  - DoubterAgent: ‚úÖ IMPLEMENTED
  - TIM: ‚úÖ IMPLEMENTED (rule-based, no LLM)
```

### –ö–∞—Ç–µ–≥–æ—Ä–∏—è C: Multi-Perspective Analysis (Debate)
```
–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- Output = structured arguments + synthesis
- Risk = MEDIUM (advisory, –Ω–µ gate)
- Technique = Role Assignment + Structured Output + Constraint
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: –∫–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –≤–∏–¥–∏—Ç –û–î–ù–ò –¥–∞–Ω–Ω—ã–µ
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: synthesizer –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–µ–Ω —á–µ–º –ª—é–±–æ–π –∞–≥–µ–Ω—Ç

APE Components: Debate System (planned Week 5-6)
Status: ‚è∏Ô∏è NOT IMPLEMENTED (Milestone 2)
```

### –ö–∞—Ç–µ–≥–æ—Ä–∏—è D: Evaluation / Judging
```
–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- Output = scores + reasoning
- Risk = LOW (–º–µ—Ç—Ä–∏–∫–∞, –Ω–µ —Ä–µ—à–µ–Ω–∏–µ)
- Technique = Rubric-based + Chain-of-Thought
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: explicit scoring rubric
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: reasoning BEFORE score

APE Components: Evaluation Module
Status: ‚úÖ IMPLEMENTED (ground truth comparison)
```

### –ö–∞—Ç–µ–≥–æ—Ä–∏—è E: Data Extraction / Parsing
```
–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- Output = structured data from unstructured input
- Risk = MEDIUM
- Technique = Structured Output + Examples
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: schema validation

APE Components: Truth Boundary Gate (TruthBoundaryGate)
Status: ‚úÖ IMPLEMENTED (deterministic, no LLM)
```

### –ö–∞—Ç–µ–≥–æ—Ä–∏—è F: Temporal / Regulatory Validation
```
–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- Output = temporal_valid: bool + violations[]
- Risk = HIGH (look-ahead bias = –≤—Å—ë –æ–±–Ω—É–ª—è–µ—Ç)
- Technique = Rule Injection + Checklist + Structured Output
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: hard rules (10-K = T+90, etc.) –ø—Ä—è–º–æ –≤ –ø—Ä–æ–º–ø—Ç–µ

APE Components: TemporalIntegrityChecker (TIM)
Status: ‚úÖ IMPLEMENTED (rule-based regex, no LLM)
```

---

## –£–†–û–í–ï–ù–¨ 3: PROMPT COMPOSITION (6 –±–ª–æ–∫–æ–≤)

### –ë–ª–æ–∫ 1: ROLE (–∫—Ç–æ)
```
–®–∞–±–ª–æ–Ω: "You are APE {RoleName} ‚Äî {one-sentence expertise description}."
–ü—Ä–∞–≤–∏–ª–æ: –û–î–ù–û –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –ù–µ –±–æ–ª—å—à–µ.
–ö–æ–≥–¥–∞ –Ω—É–∂–µ–Ω: Categories A, B, C (domain expertise matters)
–ö–æ–≥–¥–∞ –ù–ï –Ω—É–∂–µ–Ω: Categories D, E (generic task)
```

### –ë–ª–æ–∫ 2: TASK (—á—Ç–æ)
```
–®–∞–±–ª–æ–Ω: –ò–º–ø–µ—Ä–∞—Ç–∏–≤. "Generate..." / "Validate..." / "Compare..."
–ü—Ä–∞–≤–∏–ª–æ: –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ = action verb + object + purpose
–ö–æ–≥–¥–∞ –Ω—É–∂–µ–Ω: –í–°–ï–ì–î–ê
```

### –ë–ª–æ–∫ 3: CONSTRAINTS (—á–µ–≥–æ –Ω–∏–∫–æ–≥–¥–∞)
```
–®–∞–±–ª–æ–Ω: "## ABSOLUTE RULES\n1. NEVER...\n2. ALWAYS..."
–ü—Ä–∞–≤–∏–ª–æ: –¢–æ–ª—å–∫–æ –∑–∞–ø—Ä–µ—Ç—ã –∫–æ—Ç–æ—Ä—ã–µ –†–ï–ê–õ–¨–ù–û –Ω–∞—Ä—É—à–∞—é—Ç—Å—è –±–µ–∑ –Ω–∏—Ö
–ö–æ–≥–¥–∞ –Ω—É–∂–µ–Ω: Categories A, B, F (high-risk)
Anti-pattern: –ù–µ –¥–æ–±–∞–≤–ª—è–π constraint "–¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏" ‚Äî
  –∫–∞–∂–¥—ã–π constraint = –ø–æ—Ç–µ—Ä—è creativity
```

### –ë–ª–æ–∫ 4: INPUT FORMAT (—á—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç)
```
–®–∞–±–ª–æ–Ω: "## INPUT\nYou receive: {description of data structure}"
–ü—Ä–∞–≤–∏–ª–æ: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, —Ç–∏–ø—ã, –ø—Ä–∏–º–µ—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π
–ö–æ–≥–¥–∞ –Ω—É–∂–µ–Ω: –í–°–ï–ì–î–ê –∫–æ–≥–¥–∞ input –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
```

### –ë–ª–æ–∫ 5: OUTPUT FORMAT (—á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç)
```
–®–∞–±–ª–æ–Ω: "## OUTPUT\nReturn JSON matching {SchemaName} schema:\n{schema}"
–ü—Ä–∞–≤–∏–ª–æ: –ï—Å–ª–∏ downstream –∫–æ–¥ –ø–∞—Ä—Å–∏—Ç ‚Üí Pydantic schema –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞
–ö–æ–≥–¥–∞ –Ω—É–∂–µ–Ω: Categories A, B, C, D, E, F (–ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –≤ APE)
```

### –ë–ª–æ–∫ 6: EDGE CASES (—á—Ç–æ –¥–µ–ª–∞—Ç—å –∫–æ–≥–¥–∞ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ)
```
–®–∞–±–ª–æ–Ω: "## EDGE CASES\n- If {condition}: {action}\n- If {condition}: {action}"
–ü—Ä–∞–≤–∏–ª–æ: –¢–æ–ª—å–∫–æ –†–ï–ê–õ–¨–ù–´–ï edge cases –∏–∑ —Ç–µ—Å—Ç–æ–≤/production
Anti-pattern: –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π edge cases –∑–∞—Ä–∞–Ω–µ–µ ‚Äî –¥–æ–±–∞–≤–ª—è–π –ø–æ –º–µ—Ä–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
```

---

## –£–†–û–í–ï–ù–¨ 4: PROMPT LIFECYCLE

### Current APE Implementation Status

| Component | Lifecycle Stage | Notes |
|-----------|-----------------|-------|
| PLAN Node | v1 (hardcoded) | Need v2+ DSPy optimization |
| DoubterAgent | v1 (hardcoded) | Working, can optimize |
| TIM | N/A | Rule-based (no LLM) |
| Truth Gate | N/A | Deterministic (no LLM) |
| Evaluation | v1 | Ground truth comparison |

### Recommended Evolution

**Week 4 Day 4+**: Prompt Compiler Implementation
- Create `APEPromptCompiler` class
- Implement dynamic prompt generation for Doubter
- Add TDD tests for prompts
- DSPy optimization for PLAN Node (Week 5)

---

## –£–†–û–í–ï–ù–¨ 5: ANTI-PATTERNS (—Ç–µ–∫—É—â–∏–π –ø—Ä–æ–µ–∫—Ç)

### ‚úÖ What We're Doing Right

1. **Structured Output Everywhere**: All LLM nodes use Pydantic schemas
2. **TDD for Prompts**: Tests validate LLM outputs before implementation
3. **Minimal Prompts**: Current prompts are concise (1-2 paragraphs)
4. **Deterministic Where Possible**: Gate and TIM use code, not LLM

### ‚ùå Current Weaknesses (to fix)

1. **Hardcoded Prompts**: PLAN Node prompt is hardcoded string
   - **Fix**: Implement `APEPromptCompiler` (Week 4 Day 4)

2. **No Prompt Versioning**: Can't A/B test prompts
   - **Fix**: Store prompts in config/prompts/ directory

3. **No DSPy Optimization**: PLAN Node not optimized
   - **Fix**: Week 5 - DSPy optimization session

4. **No Edge Case Handling**: Prompts don't handle ambiguous queries
   - **Fix**: Add –ë–ª–æ–∫ 6 (EDGE CASES) to all prompts

---

## –£–†–û–í–ï–ù–¨ 6: APE LANGGRAPH INTEGRATION

### Recommended Architecture

```python
# src/prompt_compiler/compiler.py
class APEPromptCompiler:
    def __init__(self):
        self.meta_prompt = META_PROMPT
        self.taxonomy = TaskTaxonomy()  # 6 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.blocks = PromptBlocks()    # 6 –±–ª–æ–∫–æ–≤
        self.cache = {}                 # compiled prompts cache

    def compile(self, task_description: str, context: dict) -> str:
        # 1. Classify –∑–∞–¥–∞—á—É
        task_type = self.taxonomy.classify(task_description)

        # 2. –í—ã–±—Ä–∞—Ç—å –Ω—É–∂–Ω—ã–µ –±–ª–æ–∫–∏
        required_blocks = self.taxonomy.get_blocks(task_type)

        # 3. –ó–∞–ø–æ–ª–Ω–∏—Ç—å –±–ª–æ–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        filled_blocks = []
        for block in required_blocks:
            filled = block.fill(context)
            filled_blocks.append(filled)

        # 4. –°–æ–±—Ä–∞—Ç—å prompt
        prompt = "\n\n".join(filled_blocks)

        # 5. Validate
        self.validate(prompt, task_type)

        # 6. Cache
        cache_key = hash(task_description + str(context))
        self.cache[cache_key] = prompt

        return prompt
```

### LangGraph Node Integration (Example)

```python
def plan_node(state: APEState, config: RunnableConfig) -> APEState:
    compiler = APEPromptCompiler()

    # Compile prompt –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    system_prompt = compiler.compile(
        task_description="Generate executable Python code for financial analysis",
        context={
            "query": state.query_text,
            "tickers": state.available_tickers,
            "date_range": (state.start_date, state.end_date),
            "task_type": "code_generation",
        }
    )

    # Invoke Claude with compiled prompt
    response = claude.messages.create(
        system=system_prompt,
        messages=[{"role": "user", "content": state.query_text}]
    )

    state.plan = response.content
    return state
```

---

## HARDCODED vs COMPILED ‚Äî –¢–µ–∫—É—â–∞—è –°—Ç—Ä–∞—Ç–µ–≥–∏—è

| Component | Current | Target | Priority |
|-----------|---------|--------|----------|
| PLAN Node | Hardcoded v1 | DSPy v2+ | üî¥ HIGH (Week 5) |
| Truth Boundary | N/A (code) | N/A (keep) | ‚úÖ Done |
| VEE Sandbox | N/A (code) | N/A (keep) | ‚úÖ Done |
| Doubter Agent | Hardcoded v1 | Compiled | üü° MEDIUM (Week 4 Day 4) |
| Debate agents | Not impl | Semi-hard | ‚è∏Ô∏è Week 5-6 |
| TIM | N/A (regex) | Hardcoded rules | ‚úÖ Done |

---

## –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ï HARDCODED –ü–†–û–ú–ü–¢–´ (–¶–µ–ª–µ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ)

1. **META_PROMPT** ‚Äî —Å–∞–º –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä (–º–µ–Ω—è–µ—Ç—Å—è —Ä–µ–¥–∫–æ)
2. **PLAN_SYSTEM_PROMPT** ‚Äî —è–¥—Ä–æ pipeline (–ø–æ—Å–ª–µ v2+ DSPy optimization)
3. **TEMPORAL_RULES** ‚Äî —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ (10-K=T+90, –Ω–µ –º–µ–Ω—è—é—Ç—Å—è)

–í—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî COMPILED –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏.

---

## Immediate Action Items (Week 4 Day 4)

### High Priority
1. ‚úÖ Integrate this methodology into memory bank
2. üî¥ Create `APEPromptCompiler` skeleton (Week 4 Day 4-5)
3. üî¥ Refactor DoubterAgent to use compiled prompts
4. üü° Add prompt versioning system (config/prompts/)

### Medium Priority (Week 5)
5. DSPy optimization for PLAN Node
6. Add EDGE CASES block to all prompts
7. Implement Debate system (Category C)

### Low Priority (Week 6+)
8. A/B testing infrastructure for prompts
9. Prompt analytics dashboard
10. Production feedback loop automation

---

## References

- Original: `C:\Users\serge\Downloads\ape_prompt_methodology.md`
- Integration Date: 2026-02-08
- Integrated By: Claude Sonnet 4.5 (Autonomous Session)
- Next Review: Week 5 Day 1 (before DSPy optimization)

---

*This is a living document. Update as methodology evolves.*
