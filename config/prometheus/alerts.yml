# Prometheus Alert Rules for APE 2026
# Week 2 Day 6-7: Production Readiness

groups:
  - name: ape_critical
    interval: 30s
    rules:
      # Critical: Accuracy dropped below 90%
      - alert: AccuracyCritical
        expr: ape_accuracy{window="1h"} < 0.90
        for: 2m
        labels:
          severity: critical
          category: quality
        annotations:
          summary: "Prediction accuracy critical"
          description: "Accuracy dropped to {{ $value | humanizePercentage }} (threshold: 90%)"
          runbook_url: "https://wiki.internal/ape/runbooks/accuracy-drop"
          action: "Check Golden Set results, verify LLM provider health"

      # Critical: High error rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(ape_queries_total{status="error"}[5m]))
            /
            sum(rate(ape_queries_total[5m]))
          ) > 0.10
        for: 1m
        labels:
          severity: critical
          category: reliability
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 10%)"
          action: "Check logs for errors, verify database connections"

      # Critical: LLM provider failures
      - alert: LLMProviderDown
        expr: |
          (
            sum(rate(ape_llm_calls_total{status!="success"}[5m]))
            /
            sum(rate(ape_llm_calls_total[5m]))
          ) > 0.50
        for: 2m
        labels:
          severity: critical
          category: external_dependency
        annotations:
          summary: "LLM provider experiencing high failure rate"
          description: "LLM failure rate is {{ $value | humanizePercentage }}"
          action: "Check fallback provider, verify API keys"

  - name: ape_warning
    interval: 1m
    rules:
      # Warning: Slow queries
      - alert: SlowQueries
        expr: |
          (
            sum(rate(ape_query_duration_seconds_bucket{le="60"}[5m]))
            /
            sum(rate(ape_query_duration_seconds_count[5m]))
          ) < 0.95
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Query latency elevated"
          description: "Less than 95% of queries complete within 60s"
          action: "Check VEE sandbox performance, optimize timeouts"

      # Warning: High LLM latency
      - alert: HighLLMLatency
        expr: histogram_quantile(0.95, sum(rate(ape_llm_latency_seconds_bucket[5m])) by (le)) > 30
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "LLM P95 latency high"
          description: "LLM P95 latency is {{ $value }}s (threshold: 30s)"
          action: "Consider switching provider or enabling caching"

      # Warning: WebSocket connections dropping
      - alert: WebSocketConnectionDrops
        expr: |
          (
            sum(rate(ape_websocket_disconnections_total[5m]))
            >
            sum(rate(ape_websocket_connections_total[5m])) * 0.1
          )
        for: 5m
        labels:
          severity: warning
          category: reliability
        annotations:
          summary: "WebSocket connections unstable"
          description: "High disconnection rate detected"
          action: "Check Redis connection, verify network stability"

      # Warning: Cache hit rate low
      - alert: LowCacheHitRate
        expr: |
          (
            sum(rate(ape_cache_hits_total[5m]))
            /
            (sum(rate(ape_cache_hits_total[5m])) + sum(rate(ape_cache_misses_total[5m])))
          ) < 0.50
        for: 10m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Cache hit rate low"
          description: "Cache hit rate is {{ $value | humanizePercentage }}"
          action: "Review cache TTL settings, check for cache invalidation issues"

  - name: ape_info
    interval: 5m
    rules:
      # Info: VEE sandbox restarts
      - alert: VEESandboxRestarts
        expr: rate(ape_vee_sandbox_restarts_total[15m]) > 0
        for: 0m
        labels:
          severity: info
          category: operational
        annotations:
          summary: "VEE sandbox restarted"
          description: "Sandbox container was restarted"
          action: "Informational - may indicate memory pressure"

      # Info: Deployment completed
      - alert: DeploymentCompleted
        expr: changes(ape_app_info{version=~".+"}[5m]) > 0
        for: 0m
        labels:
          severity: info
          category: deployment
        annotations:
          summary: "New deployment detected"
          description: "APE version {{ $labels.version }} deployed"
          action: "Monitor for anomalies in next 30 minutes"

# Alert Routing Configuration (for Alertmanager)
# This would typically be in alertmanager.yml
# 
# route:
#   group_by: ['alertname', 'severity']
#   group_wait: 30s
#   group_interval: 5m
#   repeat_interval: 4h
#   receiver: 'default'
#   routes:
#     - match:
#         severity: critical
#       receiver: 'pagerduty'
#       continue: true
#     - match:
#         severity: warning
#       receiver: 'slack'
#
# receivers:
#   - name: 'default'
#     slack_configs:
#       - api_url: '${SLACK_WEBHOOK_URL}'
#   - name: 'pagerduty'
#     pagerduty_configs:
#       - service_key: '${PAGERDUTY_KEY}'
